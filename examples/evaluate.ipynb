{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from overcast import models\n",
    "from overcast import datasets\n",
    "from overcast.models import ensembles\n",
    "from overcast.visualization import plotting\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rc = {\n",
    "    \"figure.constrained_layout.use\": True,\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"legend.frameon\": True,\n",
    "    \"figure.figsize\": (6, 6),\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"legend.title_fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "}\n",
    "_ = sns.set(style=\"whitegrid\", palette=\"colorblind\", rc=rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"/users/ms21mmso/msc-project/overcast/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9\",\n",
    "# \"/users/ms21mmso/msc-project/overcast/output/jasmin-four_outputs_liqcf_atlantic_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-96_nco-3_nct-9_dp-5_ns-0.11_bt-0.0_ln-False_dr-0.04_sn-0.0_lr-0.0002_bs-224_ep-9\",\n",
    "# \"/users/ms21mmso/msc-project/overcast/output/jasmin-daily-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-transformer/dh-128_nco-22_nct-27_dp-3_nh-8_ns-0.28_bt-0.0_ln-False_dr-0.42_sn-0.0_lr-0.0001_bs-128_ep-500\",\n",
    "# \"/users/ms21mmso/msc-project/overcast/output/jasmin-daily-four_outputs_liqcf_atlantic_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-transformer/dh-128_nco-24_nct-7_dp-4_nh-8_ns-0.19_bt-0.0_ln-True_dr-0.16_sn-0.0_lr-0.0001_bs-160_ep-500\",\n",
    "# \"/users/ms21mmso/msc-project/overcast/output/jasmin-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-nn/dh-256_nco-24_nct-24_dp-3_ns-0.04_bt-0.0_ln-False_dr-0.2_sn-0.0_lr-0.0001_bs-2048_ep-300\"\n",
    "# \"/users/ms21mmso/msc-project/overcast/output/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500\"\n",
    "\n",
    "experiment = \"jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1\"\n",
    "transformer = True if \"daily\" in experiment else False\n",
    "parameters = \"dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500\"\n",
    "appended = \"appended-treatment-transformer\" if transformer else \"appended-treatment-nn\"\n",
    "\n",
    "experiment_dir = Path(f\"/scratch/ms21mmso/output/nice/{experiment}/{appended}/{parameters}\")\n",
    "config_path = experiment_dir / \"config.json\"\n",
    "checkpoint_dir = experiment_dir / \"checkpoints\"\n",
    "\n",
    "ensemble_dir = experiment_dir\n",
    "fig_dir = Path(\"/users/ms21mmso/msc-project/overcast/results\")\n",
    "\n",
    "savefig = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35857, 6)\n",
      "(32691, 6)\n",
      "(16682, 6)\n",
      "(15057, 6)\n",
      "(25760, 6)\n",
      "(22987, 6)\n",
      "(23172, 6)\n",
      "(17107, 6)\n",
      "(15990, 6)\n",
      "(18986, 6)\n",
      "(17813, 6)\n",
      "(17330, 6)\n",
      "(15933, 6)\n",
      "(27544, 6)\n",
      "(17093, 6)\n",
      "(14597, 6)\n",
      "(16774, 6)\n",
      "(19507, 6)\n",
      "(14222, 6)\n",
      "(14771, 6)\n",
      "(14993, 6)\n",
      "(15059, 6)\n",
      "(17142, 6)\n",
      "(13404, 6)\n",
      "(16554, 6)\n",
      "(17050, 6)\n",
      "(16331, 6)\n",
      "(19047, 6)\n",
      "(19233, 6)\n",
      "(20308, 6)\n",
      "(24420, 6)\n",
      "(17881, 6)\n",
      "(17754, 6)\n",
      "(26265, 6)\n",
      "(23658, 6)\n",
      "(18318, 6)\n",
      "(19216, 6)\n",
      "(17167, 6)\n",
      "(19331, 6)\n",
      "(24682, 6)\n",
      "(18696, 6)\n",
      "(17026, 6)\n",
      "(17028, 6)\n",
      "(19768, 6)\n",
      "(17385, 6)\n",
      "(16709, 6)\n",
      "(18966, 6)\n",
      "(16377, 6)\n",
      "(19577, 6)\n",
      "(23457, 6)\n",
      "(23656, 6)\n",
      "(33352, 6)\n",
      "(32355, 6)\n",
      "(28333, 6)\n",
      "(33363, 6)\n",
      "(36392, 6)\n",
      "(24276, 6)\n",
      "(37896, 6)\n",
      "(37128, 6)\n",
      "(7750, 6)\n",
      "(32292, 6)\n",
      "(21971, 6)\n",
      "(18030, 6)\n",
      "(17328, 6)\n",
      "(17670, 6)\n",
      "(17947, 6)\n",
      "(24214, 6)\n",
      "(18507, 6)\n",
      "(16835, 6)\n",
      "(21937, 6)\n",
      "(17257, 6)\n",
      "(21140, 6)\n",
      "(19113, 6)\n",
      "(18010, 6)\n",
      "(20206, 6)\n",
      "(16365, 6)\n",
      "(15463, 6)\n",
      "(25494, 6)\n",
      "(19719, 6)\n",
      "(23372, 6)\n",
      "(16801, 6)\n",
      "(22351, 6)\n",
      "(18202, 6)\n",
      "(15840, 6)\n",
      "(22602, 6)\n",
      "(16211, 6)\n",
      "(21560, 6)\n",
      "(23168, 6)\n",
      "(17978, 6)\n",
      "(15975, 6)\n",
      "(26370, 6)\n",
      "(20470, 6)\n",
      "(23162, 6)\n",
      "(18147, 6)\n",
      "(16270, 6)\n",
      "(20033, 6)\n",
      "(15863, 6)\n",
      "(18556, 6)\n",
      "(19747, 6)\n",
      "(17185, 6)\n",
      "(17316, 6)\n",
      "(27346, 6)\n",
      "(17498, 6)\n",
      "(4964, 6)\n",
      "(18105, 6)\n",
      "(24326, 6)\n",
      "(22151, 6)\n",
      "(21923, 6)\n",
      "(18548, 6)\n",
      "(17799, 6)\n",
      "(18521, 6)\n",
      "(15234, 6)\n",
      "(18996, 6)\n",
      "(18630, 6)\n",
      "(16098, 6)\n",
      "(17289, 6)\n",
      "(16412, 6)\n",
      "(20288, 6)\n",
      "(19798, 6)\n",
      "(19624, 6)\n",
      "(19485, 6)\n",
      "(16000, 6)\n",
      "(16920, 6)\n",
      "(16130, 6)\n",
      "(16219, 6)\n",
      "(24697, 6)\n",
      "(16514, 6)\n",
      "(19313, 6)\n",
      "(15010, 6)\n",
      "(24209, 6)\n",
      "(17245, 6)\n",
      "(19990, 6)\n",
      "(17628, 6)\n",
      "(15946, 6)\n",
      "(22178, 6)\n",
      "(18009, 6)\n",
      "(19134, 6)\n",
      "(16881, 6)\n",
      "(17966, 6)\n",
      "(17230, 6)\n",
      "(14264, 6)\n",
      "(21366, 6)\n",
      "(17812, 6)\n",
      "(15899, 6)\n",
      "(11842, 6)\n",
      "(14844, 6)\n",
      "(17419, 6)\n",
      "(14697, 6)\n",
      "(15275, 6)\n",
      "(13224, 6)\n",
      "(13140, 6)\n",
      "(20746, 6)\n",
      "(15806, 6)\n",
      "(18418, 6)\n",
      "(15793, 6)\n",
      "(11836, 6)\n",
      "(14853, 6)\n",
      "(15094, 6)\n",
      "(20197, 6)\n",
      "(16236, 6)\n",
      "(14628, 6)\n",
      "(12831, 6)\n",
      "(15112, 6)\n",
      "(14704, 6)\n",
      "(17397, 6)\n",
      "(13695, 6)\n",
      "(15424, 6)\n",
      "(15796, 6)\n",
      "(17966, 6)\n",
      "(13729, 6)\n",
      "(16570, 6)\n",
      "(15881, 6)\n",
      "(12738, 6)\n",
      "(12475, 6)\n",
      "(18413, 6)\n",
      "(14765, 6)\n",
      "(17477, 6)\n",
      "(15009, 6)\n",
      "(15569, 6)\n",
      "(14928, 6)\n",
      "(19858, 6)\n",
      "(13095, 6)\n",
      "(19373, 6)\n",
      "(15052, 6)\n",
      "(12931, 6)\n",
      "(14566, 6)\n",
      "(13243, 6)\n",
      "(17549, 6)\n",
      "(14618, 6)\n",
      "(10874, 6)\n",
      "(15187, 6)\n",
      "(15463, 6)\n",
      "(20651, 6)\n",
      "(14104, 6)\n",
      "(16068, 6)\n",
      "(15324, 6)\n",
      "(16877, 6)\n",
      "(20360, 6)\n",
      "(16266, 6)\n",
      "(16201, 6)\n",
      "(16492, 6)\n",
      "(18985, 6)\n",
      "(18180, 6)\n",
      "(19079, 6)\n",
      "(17085, 6)\n",
      "(22288, 6)\n",
      "(22756, 6)\n",
      "(17679, 6)\n",
      "(19937, 6)\n",
      "(15911, 6)\n",
      "(19193, 6)\n",
      "(15761, 6)\n",
      "(18925, 6)\n",
      "(19420, 6)\n",
      "(25388, 6)\n",
      "(17903, 6)\n",
      "(16619, 6)\n",
      "(18460, 6)\n",
      "(15398, 6)\n",
      "(25211, 6)\n",
      "(17765, 6)\n",
      "(17239, 6)\n",
      "(17500, 6)\n",
      "(16109, 6)\n",
      "(28594, 6)\n",
      "(27918, 6)\n",
      "(16316, 6)\n",
      "(20575, 6)\n",
      "(16557, 6)\n",
      "(24036, 6)\n",
      "(18582, 6)\n",
      "(18257, 6)\n",
      "(18700, 6)\n",
      "(17651, 6)\n",
      "(16184, 6)\n",
      "(21587, 6)\n",
      "(19430, 6)\n",
      "(17834, 6)\n",
      "(22435, 6)\n",
      "(16421, 6)\n",
      "(25110, 6)\n",
      "(18617, 6)\n",
      "(20096, 6)\n",
      "(17319, 6)\n",
      "(17083, 6)\n",
      "(16339, 6)\n",
      "(24404, 6)\n",
      "(23677, 6)\n",
      "(17801, 6)\n",
      "(29370, 6)\n",
      "(18391, 6)\n",
      "(16231, 6)\n",
      "(20625, 6)\n",
      "(25444, 6)\n",
      "(20082, 6)\n",
      "(19078, 6)\n",
      "(20449, 6)\n",
      "(17132, 6)\n",
      "(16615, 6)\n",
      "(16231, 6)\n",
      "(25475, 6)\n",
      "(17560, 6)\n",
      "(24242, 6)\n",
      "(16206, 6)\n",
      "(21551, 6)\n",
      "(17293, 6)\n",
      "(23953, 6)\n",
      "(24659, 6)\n",
      "(17406, 6)\n",
      "(15421, 6)\n",
      "(16396, 6)\n",
      "(26450, 6)\n",
      "(19619, 6)\n",
      "(17370, 6)\n",
      "(18219, 6)\n",
      "(22216, 6)\n",
      "(17626, 6)\n",
      "(19315, 6)\n",
      "(22927, 6)\n",
      "(19650, 6)\n",
      "(21141, 6)\n",
      "(21132, 6)\n",
      "(17884, 6)\n",
      "(10992, 6)\n",
      "(8527, 6)\n",
      "(9177, 6)\n",
      "(8653, 6)\n",
      "(8115, 6)\n",
      "(9658, 6)\n",
      "(19049, 6)\n",
      "(16016, 6)\n",
      "(22157, 6)\n",
      "(15883, 6)\n",
      "(20177, 6)\n",
      "(20347, 6)\n",
      "(17234, 6)\n",
      "(17280, 6)\n",
      "(14981, 6)\n",
      "(21291, 6)\n",
      "(19797, 6)\n",
      "(19307, 6)\n",
      "(22948, 6)\n",
      "(17438, 6)\n",
      "(16714, 6)\n",
      "(21172, 6)\n",
      "(21157, 6)\n",
      "(17794, 6)\n",
      "(18618, 6)\n",
      "(19591, 6)\n",
      "(16446, 6)\n"
     ]
    }
   ],
   "source": [
    "with open(config_path) as cp:\n",
    "    config = json.load(cp)\n",
    "\n",
    "dataset_name = config.get(\"dataset_name\")\n",
    "num_components_outcome = config.get(\"num_components_outcome\")\n",
    "num_components_treatment = config.get(\"num_components_treatment\")\n",
    "dim_hidden = config.get(\"dim_hidden\")\n",
    "depth = config.get(\"depth\")\n",
    "negative_slope = config.get(\"negative_slope\")\n",
    "beta = config.get(\"beta\")\n",
    "layer_norm = config.get(\"layer_norm\")\n",
    "dropout_rate = config.get(\"dropout_rate\")\n",
    "spectral_norm = config.get(\"spectral_norm\")\n",
    "learning_rate = config.get(\"learning_rate\")\n",
    "batch_size = config.get(\"batch_size\")\n",
    "epochs = config.get(\"epochs\")\n",
    "ensemble_size = config.get(\"ensemble_size\")\n",
    "num_heads = config.get(\"num_heads\") if transformer is True else None\n",
    "\n",
    "ds = {\n",
    "    \"test\": datasets.DATASETS.get(dataset_name)(**config.get(\"ds_test\")),\n",
    "    # \"valid\": datasets.DATASETS.get(dataset_name)(**config.get(\"ds_valid\")),\n",
    "    \"train\": datasets.DATASETS.get(dataset_name)(**config.get(\"ds_train\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2709385/3946644506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTARGET_KEYS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGET_KEYS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "TARGET_KEYS = dict((k, v) for (k,v) in enumerate(ds[\"test\"].target_names))\n",
    "print(TARGET_KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 11:37:16,394 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-0/mu/best_checkpoint.pt\n",
      "/users/ms21mmso/miniconda3/envs/overcast/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "2022-08-04 11:37:19,649 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-0/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:20,596 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-1/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:23,626 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-1/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:24,669 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-2/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:27,799 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-2/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:28,812 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-3/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:32,006 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-3/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:33,104 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-4/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:36,290 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-4/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:37,412 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-5/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:40,694 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-5/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:41,823 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-6/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:45,230 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-6/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:46,364 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-7/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:49,714 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-7/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:50,802 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-8/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:54,158 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-8/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:55,242 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-9/mu/best_checkpoint.pt\n",
      "2022-08-04 11:37:58,519 overcast.models.core.AppendedTreatmentAttentionNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/nice/jasmin-daily-MERRA_25kmres_2003_treatment-AOD_covariates-RH950-RH850-RH700-LTS-W500-SST_outcomes-re-COD-CWP_bins-1/appended-treatment-transformer/dh-256_nco-2_nct-30_dp-3_nh-8_ns-0.24_bt-0.0_ln-True_dr-0.04_sn-0.0_lr-0.0007000000000000001_bs-64_ep-500/checkpoints/model-9/mu/best_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "if transformer: \n",
    "    ensemble = []\n",
    "    for ensemble_id in range(ensemble_size):\n",
    "        model_dir = checkpoint_dir / f\"model-{ensemble_id}\" / \"mu\"\n",
    "        model = models.AppendedTreatmentAttentionNetwork(\n",
    "            job_dir=model_dir,\n",
    "            dim_input=ds[\"train\"].dim_input,\n",
    "            dim_treatment=ds[\"train\"].dim_treatments,\n",
    "            dim_output=ds[\"train\"].dim_targets,\n",
    "            num_components_outcome=num_components_outcome,\n",
    "            num_components_treatment=num_components_treatment,\n",
    "            dim_hidden=dim_hidden,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            negative_slope=negative_slope,\n",
    "            beta=beta,\n",
    "            layer_norm=layer_norm,\n",
    "            spectral_norm=spectral_norm,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_examples=len(ds[\"train\"]),\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            patience=50,\n",
    "            num_workers=0,\n",
    "            seed=ensemble_id,\n",
    "        )\n",
    "        model.load()\n",
    "        ensemble.append(model)\n",
    "else:\n",
    "    ensemble = []\n",
    "    for ensemble_id in range(ensemble_size):\n",
    "        model_dir = checkpoint_dir / f\"model-{ensemble_id}\" / \"mu\"\n",
    "        model = models.AppendedTreatmentNeuralNetwork(\n",
    "            job_dir=model_dir,\n",
    "            architecture=\"resnet\",\n",
    "            dim_input=ds[\"train\"].dim_input,\n",
    "            dim_treatment=ds[\"train\"].dim_treatments,\n",
    "            dim_output=ds[\"train\"].dim_targets,\n",
    "            num_components_outcome=num_components_outcome,\n",
    "            num_components_treatment=num_components_treatment,\n",
    "            dim_hidden=dim_hidden,\n",
    "            depth=depth,\n",
    "            negative_slope=negative_slope,\n",
    "            beta=beta,\n",
    "            layer_norm=layer_norm,\n",
    "            spectral_norm=spectral_norm,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_examples=len(ds[\"train\"]),\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            patience=epochs,\n",
    "            num_workers=0,\n",
    "            seed=ensemble_id,\n",
    "        )\n",
    "        model.load()\n",
    "        ensemble.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 38.32 GiB (GPU 0; 10.92 GiB total capacity; 1.72 GiB already allocated; 8.38 GiB free; 1.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2709385/2386799127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeans_ensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensembles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mobserved_outcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/models/ensembles.py\u001b[0m in \u001b[0;36mpredict_mean\u001b[0;34m(ensemble, dataset, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m         mean_ensemble.append(\n\u001b[1;32m      8\u001b[0m             np.expand_dims(\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             )\n\u001b[1;32m     11\u001b[0m         )\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/models/attention.py\u001b[0m in \u001b[0;36mpredict_mean\u001b[0;34m(self, dataset, batch_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0moutputs_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 ) = self.preprocess(batch)\n\u001b[0;32m--> 261\u001b[0;31m                 y_density = self.network(\n\u001b[0m\u001b[1;32m    262\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mtreatments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtreatments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/overcast/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, treatments, position, inputs_mask, outputs_mask)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0moutputs_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ) -> torch.distributions.Distribution:\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mtreatments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreatments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/overcast/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, position, mask)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mencoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/overcast/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, mask)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/overcast/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/overcast/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0msplit_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_channels\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         mha = self.attention(\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/overcast/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/modules/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     ) -> torch.Tensor:\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 38.32 GiB (GPU 0; 10.92 GiB total capacity; 1.72 GiB already allocated; 8.38 GiB free; 1.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "means_ensemble = ensembles.predict_mean(ensemble, ds[\"test\"], batch_size=None)\n",
    "if transformer: \n",
    "    df_test = ds[\"test\"].data_frame\n",
    "    observed_outcomes = df_test.to_numpy()[:, -4:]\n",
    "else: \n",
    "    observed_outcomes = ds[\"test\"].targets_xfm.inverse_transform(ds[\"test\"].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for idx_outcome in range(len(TARGET_KEYS)):\n",
    "    # if idx_outcome == 4:\n",
    "    #     pass\n",
    "    qs = np.quantile(observed_outcomes[:, idx_outcome], [0.01, 0.99])\n",
    "    domain = np.arange(qs[0], qs[1], 0.01)\n",
    "    slope, intercept, r, p, stderr = stats.linregress(\n",
    "        observed_outcomes[:, idx_outcome], means_ensemble.mean(0)[:, idx_outcome]\n",
    "    )\n",
    "    i, j = idx_outcome//2, idx_outcome%2\n",
    "    _ = sns.scatterplot(x=observed_outcomes[:, idx_outcome], y=means_ensemble.mean(0)[:, idx_outcome], s=0.5, ax=ax[i][j])\n",
    "    _ = ax[i][j].plot(domain, domain, c=\"C1\")\n",
    "    _ = ax[i][j].plot(domain, domain * slope + intercept, c=\"C2\", label=f\"$r^2$={r**2:.03f}\")\n",
    "    _ = ax[i][j].set_xlim(qs)\n",
    "    _ = ax[i][j].set_ylim(qs)\n",
    "    _ = ax[i][j].set_xlabel(f\"{TARGET_KEYS[idx_outcome]} true\")\n",
    "    _ = ax[i][j].set_ylabel(f\"{TARGET_KEYS[idx_outcome]} predicted\")\n",
    "    _ = ax[i][j].legend(loc=\"upper left\")\n",
    "if savefig: \n",
    "    plt.savefig(f\"{fig_dir}/{experiment}-{parameters}-scatter.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Lambda  \\to 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformer:\n",
    "    treatments = np.concatenate(ds[\"train\"].treatments, axis=0)\n",
    "    treatments = ds[\"train\"].treatments_xfm.inverse_transform(treatments)\n",
    "    treatments = np.quantile(\n",
    "        treatments,\n",
    "        q=np.arange(0, 1 + 1 / 32, 1 / 32),\n",
    "    )[:-1]\n",
    "else: \n",
    "    treatments = np.quantile(\n",
    "        ds[\"train\"].treatments_xfm.inverse_transform(ds[\"train\"].treatments),\n",
    "        q=np.arange(0, 1 + 1 / 32, 1 / 32),\n",
    "    )[:-1]\n",
    "apos_ensemble_path = ensemble_dir / \"apos_ensemble.npy\"\n",
    "if not apos_ensemble_path.exists():\n",
    "    capos_ensemble = ensembles.predict_capos(\n",
    "        ensemble=ensemble, \n",
    "        dataset=ds[\"test\"], \n",
    "        treatments=treatments, \n",
    "        batch_size=1 if transformer else 20000,\n",
    "    )\n",
    "    apos_ensemble = capos_ensemble.mean(2)\n",
    "    np.save(apos_ensemble_path, apos_ensemble)\n",
    "else:\n",
    "    apos_ensemble = np.load(apos_ensemble_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "_, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for idx_outcome in range(len(TARGET_KEYS)):\n",
    "    # if idx_outcome == 4:\n",
    "    #     pass\n",
    "    i, j = idx_outcome//2, idx_outcome%2\n",
    "    _ = sns.lineplot(x=treatments, y=apos_ensemble[idx_outcome].mean(0), ax=ax[i][j])\n",
    "    _ = ax[i][j].fill_between(\n",
    "        x=treatments,\n",
    "        y1=np.quantile(apos_ensemble[idx_outcome], 1 - alpha / 2, axis=0),\n",
    "        y2=np.quantile(apos_ensemble[idx_outcome], alpha / 2, axis=0),\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\Lambda \\to 1.0 $\",\n",
    "    )\n",
    "    _ = ax[i][j].legend(\n",
    "        title=r\"$\\alpha=$\" + f\"{alpha}\",\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    _ = ax[i][j].set_xlabel(ds['train'].treatment_names[0])\n",
    "    _ = ax[i][j].set_ylabel(TARGET_KEYS[idx_outcome])\n",
    "if savefig: \n",
    "    plt.savefig(f\"{fig_dir}/{experiment}-{parameters}-apo.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('overcast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1da42b996cff2d6e0dd5b53bfb8eb8e8da478a613ef2589a7cb7852cd9dd31a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
