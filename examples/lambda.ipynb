{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from overcast import models\n",
    "from overcast import datasets\n",
    "from overcast.models import ensembles\n",
    "from overcast.visualization import plotting\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rc = {\n",
    "    \"figure.constrained_layout.use\": True,\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"legend.frameon\": True,\n",
    "    \"figure.figsize\": (6, 6),\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"legend.title_fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "}\n",
    "_ = sns.set(style=\"whitegrid\", palette=\"colorblind\", rc=rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dict = {\n",
    "    'pacific': Path(\"/scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9\"), \n",
    "    'atlantic': Path(\"/scratch/ms21mmso/output/jasmin-four_outputs_liqcf_atlantic_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-96_nco-3_nct-9_dp-5_ns-0.11_bt-0.0_ln-False_dr-0.04_sn-0.0_lr-0.0002_bs-224_ep-9\")\n",
    "}\n",
    "\n",
    "ds_dict = {}\n",
    "treatments_dict = {}\n",
    "ensemble_dict = {}\n",
    "apos_ensemble_dict = {}\n",
    "apo_limits_infty_dict = {}\n",
    "apo_limits_1_dict = {}\n",
    "apo_limits_2_dict = {}\n",
    "target_keys_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 18:03:16,981 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-0/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:17,266 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-0/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:17,370 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-1/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:17,659 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-1/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:17,749 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-2/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:18,045 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-2/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:18,139 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-3/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:18,396 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-3/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:18,491 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-4/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:18,815 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-4/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:18,918 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-5/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,162 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-5/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,265 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-6/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,382 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-6/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,484 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-7/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,661 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-7/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,756 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-8/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,873 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-8/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:19,968 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-9/mu/best_checkpoint.pt\n",
      "2022-07-25 18:03:20,080 overcast.models.core.AppendedTreatmentNeuralNetwork INFO: Loading saved checkpoint /scratch/ms21mmso/output/jasmin-four_outputs_liqcf_pacific_treatment-AOD_covariates-RH900-RH850-RH700-LTS-EIS-W500-SST_outcomes-re-COD-CWP-LPC_bins-1/appended-treatment-nn/dh-256_nco-5_nct-2_dp-2_ns-0.1_bt-0.0_ln-False_dr-0.09_sn-0.0_lr-0.0002_bs-224_ep-9/checkpoints/model-9/mu/best_checkpoint.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 10.92 GiB total capacity; 4.65 GiB already allocated; 1.36 GiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_940608/2703892344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mapo_limits_infty_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"apo_limits_16.0.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mapo_limits_infty_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         lower_capos, upper_capos = ensembles.predict_intervals(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mensemble\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/models/ensembles.py\u001b[0m in \u001b[0;36mpredict_intervals\u001b[0;34m(ensemble, dataset, treatments, log_lambda, num_samples, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtreatment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtreatments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             intervals = model.predict_capo_interval(\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/auto/users/ms21mmso/msc-project/overcast/overcast/models/neural_network.py\u001b[0m in \u001b[0;36mpredict_capo_interval\u001b[0;34m(self, dataset, treatment, log_lambda, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0;31m# sweep over upper bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m  \u001b[0;31m# [num_samples, batch_size, dy]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                 d = y_samples - y_samples.unsqueeze(\n\u001b[0m\u001b[1;32m    728\u001b[0m                     \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 )  # [num_samples, num_samples, batch_size, dy]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 10.92 GiB total capacity; 4.65 GiB already allocated; 1.36 GiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for k, v in experiment_dict.items():\n",
    "    experiment_dir = v\n",
    "    config_path = experiment_dir / \"config.json\"\n",
    "    checkpoint_dir = experiment_dir / \"checkpoints\"\n",
    "    ensemble_dir = experiment_dir\n",
    "    transformer = False\n",
    "\n",
    "    with open(config_path) as cp:\n",
    "        config = json.load(cp)\n",
    "\n",
    "    dataset_name = config.get(\"dataset_name\")\n",
    "    num_components_outcome = config.get(\"num_components_outcome\")\n",
    "    num_components_treatment = config.get(\"num_components_treatment\")\n",
    "    dim_hidden = config.get(\"dim_hidden\")\n",
    "    depth = config.get(\"depth\")\n",
    "    negative_slope = config.get(\"negative_slope\")\n",
    "    beta = config.get(\"beta\")\n",
    "    layer_norm = config.get(\"layer_norm\")\n",
    "    dropout_rate = config.get(\"dropout_rate\")\n",
    "    spectral_norm = config.get(\"spectral_norm\")\n",
    "    learning_rate = config.get(\"learning_rate\")\n",
    "    batch_size = config.get(\"batch_size\")\n",
    "    epochs = config.get(\"epochs\")\n",
    "    ensemble_size = config.get(\"ensemble_size\")\n",
    "    num_heads = config.get(\"num_heads\") if transformer is True else None\n",
    "\n",
    "    ds = {\n",
    "        \"test\": datasets.DATASETS.get(dataset_name)(**config.get(\"ds_test\")),\n",
    "        \"valid\": datasets.DATASETS.get(dataset_name)(**config.get(\"ds_valid\")),\n",
    "        \"train\": datasets.DATASETS.get(dataset_name)(**config.get(\"ds_train\")),\n",
    "    }\n",
    "\n",
    "    target_keys = dict((k, v) for (k,v) in enumerate(ds[\"test\"].target_names))\n",
    "\n",
    "    if transformer: \n",
    "        ensemble = []\n",
    "        for ensemble_id in range(ensemble_size):\n",
    "            model_dir = checkpoint_dir / f\"model-{ensemble_id}\" / \"mu\"\n",
    "            model = models.AppendedTreatmentAttentionNetwork(\n",
    "                job_dir=model_dir,\n",
    "                dim_input=ds[\"train\"].dim_input,\n",
    "                dim_treatment=ds[\"train\"].dim_treatments,\n",
    "                dim_output=ds[\"train\"].dim_targets,\n",
    "                num_components_outcome=num_components_outcome,\n",
    "                num_components_treatment=num_components_treatment,\n",
    "                dim_hidden=dim_hidden,\n",
    "                depth=depth,\n",
    "                num_heads=num_heads,\n",
    "                negative_slope=negative_slope,\n",
    "                beta=beta,\n",
    "                layer_norm=layer_norm,\n",
    "                spectral_norm=spectral_norm,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_examples=len(ds[\"train\"]),\n",
    "                learning_rate=learning_rate,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                patience=50,\n",
    "                num_workers=0,\n",
    "                seed=ensemble_id,\n",
    "            )\n",
    "            model.load()\n",
    "            ensemble.append(model)\n",
    "    else:\n",
    "        ensemble = []\n",
    "        for ensemble_id in range(ensemble_size):\n",
    "            model_dir = checkpoint_dir / f\"model-{ensemble_id}\" / \"mu\"\n",
    "            model = models.AppendedTreatmentNeuralNetwork(\n",
    "                job_dir=model_dir,\n",
    "                architecture=\"resnet\",\n",
    "                dim_input=ds[\"train\"].dim_input,\n",
    "                dim_treatment=ds[\"train\"].dim_treatments,\n",
    "                dim_output=ds[\"train\"].dim_targets,\n",
    "                num_components_outcome=num_components_outcome,\n",
    "                num_components_treatment=num_components_treatment,\n",
    "                dim_hidden=dim_hidden,\n",
    "                depth=depth,\n",
    "                negative_slope=negative_slope,\n",
    "                beta=beta,\n",
    "                layer_norm=layer_norm,\n",
    "                spectral_norm=spectral_norm,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_examples=len(ds[\"train\"]),\n",
    "                learning_rate=learning_rate,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                patience=epochs,\n",
    "                num_workers=0,\n",
    "                seed=ensemble_id,\n",
    "            )\n",
    "            model.load()\n",
    "            ensemble.append(model)\n",
    "            \n",
    "    if transformer:\n",
    "        treatments = np.concatenate(ds[\"train\"].treatments, axis=0)\n",
    "        treatments = ds[\"train\"].treatments_xfm.inverse_transform(treatments)\n",
    "        treatments = np.quantile(\n",
    "            treatments,\n",
    "            q=np.arange(0, 1 + 1 / 32, 1 / 32),\n",
    "        )[:-1]\n",
    "    else: \n",
    "        treatments = np.quantile(\n",
    "            ds[\"train\"].treatments_xfm.inverse_transform(ds[\"train\"].treatments),\n",
    "            q=np.arange(0, 1 + 1 / 32, 1 / 32),\n",
    "        )[:-1]\n",
    "    \n",
    "    apos_ensemble_path = ensemble_dir / \"apos_ensemble.npy\"\n",
    "    if not apos_ensemble_path.exists():\n",
    "        capos_ensemble = ensembles.predict_capos(\n",
    "            ensemble=ensemble, \n",
    "            dataset=ds[\"test\"], \n",
    "            treatments=treatments, \n",
    "            batch_size=1 if transformer else 20000,\n",
    "        )\n",
    "        apos_ensemble = capos_ensemble.mean(2)\n",
    "        np.save(apos_ensemble_path, apos_ensemble)\n",
    "    else:\n",
    "        apos_ensemble = np.load(apos_ensemble_path)\n",
    "    \n",
    "    log_lambda = 16.0\n",
    "    apo_limits_infty_path = ensemble_dir / \"apo_limits_16.0.npy\"\n",
    "    if not apo_limits_infty_path.exists():\n",
    "        lower_capos, upper_capos = ensembles.predict_intervals(\n",
    "            ensemble=ensemble,\n",
    "            dataset=ds[\"test\"],\n",
    "            treatments=treatments,\n",
    "            log_lambda=log_lambda,\n",
    "            num_samples=1 if transformer else 100,\n",
    "            batch_size=1 if transformer else 10000,\n",
    "        )\n",
    "        lower_apos = np.expand_dims(lower_capos.mean(2), 0)\n",
    "        upper_apos = np.expand_dims(upper_capos.mean(2), 0)\n",
    "        apo_limits_infty = np.concatenate([lower_apos, upper_apos], axis=0)\n",
    "        np.save(apo_limits_infty_path, apo_limits_infty)\n",
    "    else:\n",
    "        apo_limits_infty = np.load(apo_limits_infty_path)\n",
    "\n",
    "    log_lambda = 0.1\n",
    "    apo_limits_1_path = ensemble_dir /  \"apo_limits_0.1.npy\"\n",
    "    if not apo_limits_1_path.exists():\n",
    "        lower_capos, upper_capos = ensembles.predict_intervals(\n",
    "            ensemble=ensemble,\n",
    "            dataset=ds[\"test\"],\n",
    "            treatments=treatments,\n",
    "            log_lambda=log_lambda,\n",
    "            num_samples=1 if transformer else 100,\n",
    "            batch_size=10000,\n",
    "        )\n",
    "        lower_apos = np.expand_dims(lower_capos.mean(2), 0)\n",
    "        upper_apos = np.expand_dims(upper_capos.mean(2), 0)\n",
    "        apo_limits_1 = np.concatenate([lower_apos, upper_apos], axis=0)\n",
    "        np.save(apo_limits_1_path, apo_limits_1)\n",
    "    else:\n",
    "        apo_limits_1 = np.load(apo_limits_1_path)\n",
    "\n",
    "    log_lambda = 0.2\n",
    "    apo_limits_2_path = ensemble_dir /  \"apo_limits_0.2.npy\"\n",
    "    if not apo_limits_2_path.exists():\n",
    "        lower_capos, upper_capos = ensembles.predict_intervals(\n",
    "            ensemble=ensemble,\n",
    "            dataset=ds[\"test\"],\n",
    "            treatments=treatments,\n",
    "            log_lambda=log_lambda,\n",
    "            num_samples=1 if transformer else 100,\n",
    "            batch_size=1 if transformer else 10000,\n",
    "        )\n",
    "        lower_apos = np.expand_dims(lower_capos.mean(2), 0)\n",
    "        upper_apos = np.expand_dims(upper_capos.mean(2), 0)\n",
    "        apo_limits_2 = np.concatenate([lower_apos, upper_apos], axis=0)\n",
    "        np.save(apo_limits_2_path, apo_limits_2)\n",
    "    else:\n",
    "        apo_limits_2 = np.load(apo_limits_2_path)\n",
    "    \n",
    "\n",
    "    ds_dict[k] = ds\n",
    "    treatments_dict[k] = treatments\n",
    "    ensemble_dict[k] = ensemble\n",
    "    apos_ensemble_dict[k] = apos_ensemble\n",
    "    apo_limits_infty_dict[k] = apo_limits_infty\n",
    "    apo_limits_1_dict[k] = apo_limits_1\n",
    "    apo_limits_2_dict[k] = apo_limits_2\n",
    "    target_keys_dict[k] = target_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Lambda  \\to 1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for idx_outcome in range(len(target_keys_dict['pacific'])):\n",
    "    i, j = idx_outcome//2, idx_outcome%2\n",
    "    for k in experiment_dict.keys():\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1))\n",
    "        _ = ax[i][j].plot(treatments_dict[k], scaler.transform(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1)), label=k)\n",
    "        if k == 'pacific':\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                label=r\"$\\Lambda \\to 1.0 $\",\n",
    "            )\n",
    "    _ = ax[i][j].legend(\n",
    "        title=r\"$\\alpha=$\" + f\"{alpha}\",\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    _ = ax[i][j].set_xlabel(ds['train'].treatment_names[0])\n",
    "    _ = ax[i][j].set_ylabel(target_keys_dict['pacific'][idx_outcome])\n",
    "plt.savefig('four_outputs_liqcf-1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Lambda \\to \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for idx_outcome in range(len(target_keys_dict['pacific'])):\n",
    "    i, j = idx_outcome//2, idx_outcome%2\n",
    "    for k in experiment_dict.keys():\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1))\n",
    "        _ = ax[i][j].plot(treatments_dict[k], scaler.transform(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1)), label=k)\n",
    "        if k == 'pacific':\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                label=r\"$\\Lambda \\to 1.0 $\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apo_limits_infty[k][1][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C8\",\n",
    "                label=r\"$\\Lambda \\to \\infty$\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments,\n",
    "                y1=scaler.transform(np.quantile(apo_limits_infty[k][0][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C8\",\n",
    "            )\n",
    "    _ = ax[i][j].legend(\n",
    "        title=r\"$\\alpha=$\" + f\"{alpha}\",\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    _ = ax[i][j].set_xlabel(ds['train'].treatment_names[0])\n",
    "    _ = ax[i][j].set_ylabel(target_keys_dict['pacific'][idx_outcome])\n",
    "plt.savefig('four_outputs_liqcf-infty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Lambda = 1.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for idx_outcome in range(len(target_keys_dict['pacific'])):\n",
    "    i, j = idx_outcome//2, idx_outcome%2\n",
    "    for k in experiment_dict.keys():\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1))\n",
    "        _ = ax[i][j].plot(treatments_dict[k], scaler.transform(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1)), label=k)\n",
    "        if k == 'pacific':\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                label=r\"$\\Lambda \\to 1.0 $\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apo_limits_1_dict[k][1][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C1\",\n",
    "                label=r\"$\\Lambda=$\" + f\"{np.exp(0.1):.01f}\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments,\n",
    "                y1=scaler.transform(np.quantile(apo_limits_1_dict[k][0][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C1\",\n",
    "            )\n",
    "    _ = ax[i][j].legend(\n",
    "        title=r\"$\\alpha=$\" + f\"{alpha}\",\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    _ = ax[i][j].set_xlabel(ds['train'].treatment_names[0])\n",
    "    _ = ax[i][j].set_ylabel(target_keys_dict['pacific'][idx_outcome])\n",
    "plt.savefig('four_outputs_liqcf-1.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Lambda = 1.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for idx_outcome in range(len(target_keys_dict['pacific'])):\n",
    "    i, j = idx_outcome//2, idx_outcome%2\n",
    "    for k in experiment_dict.keys():\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1))\n",
    "        _ = ax[i][j].plot(treatments_dict[k], scaler.transform(apos_ensemble_dict[k][idx_outcome].mean(0).reshape(-1, 1)), label=k)\n",
    "        if k == 'pacific':\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble_dict[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                label=r\"$\\Lambda \\to 1.0 $\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments_dict[k],\n",
    "                y1=scaler.transform(np.quantile(apo_limits_1_dict[k][1][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C1\",\n",
    "                label=r\"$\\Lambda=$\" + f\"{np.exp(0.1):.01f}\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments,\n",
    "                y1=scaler.transform(np.quantile(apo_limits_1_dict[k][0][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apos_ensemble[k][idx_outcome], alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C1\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments,\n",
    "                y1=scaler.transform(np.quantile(apo_limits_2_dict[k][1][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apo_limits_1_dict[k][1][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C2\",\n",
    "                label=r\"$\\Lambda=$\" + f\"{np.exp(0.2):.01f}\",\n",
    "            )\n",
    "            _ = ax[i][j].fill_between(\n",
    "                x=treatments,\n",
    "                y1=scaler.transform(np.quantile(apo_limits_2_dict[k][0][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                y2=scaler.transform(np.quantile(apo_limits_1_dict[k][0][idx_outcome], 1 - alpha / 2, axis=0).reshape(-1, 1)).flatten(),\n",
    "                alpha=0.2,\n",
    "                color=\"C2\",\n",
    "            )\n",
    "    _ = ax[i][j].legend(\n",
    "        title=r\"$\\alpha=$\" + f\"{alpha}\",\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    _ = ax[i][j].set_xlabel(ds['train'].treatment_names[0])\n",
    "    _ = ax[i][j].set_ylabel(target_keys_dict['pacific'][idx_outcome])\n",
    "plt.savefig('four_outputs_liqcf-1.2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('overcast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1da42b996cff2d6e0dd5b53bfb8eb8e8da478a613ef2589a7cb7852cd9dd31a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
